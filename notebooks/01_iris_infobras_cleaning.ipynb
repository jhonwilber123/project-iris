{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a972a3-5cec-42d2-a989-0b5d0cc5fe3a",
   "metadata": {},
   "source": [
    "# Project IRIS: INFOBRAS Data Cleaning and Certification\n",
    "\n",
    "**Lead Data Engineer:** Jhon Wilber Ajata Ascarrunz\n",
    "\n",
    "**Objective:** This notebook ingests the raw INFOBRAS public works dataset (`DataSet-Obras-Publicas-23-07-2025.xlsx`), performs a full cleaning, auditing, and curation pipeline, and exports a certified, analysis-ready data asset (`infobras_certified_v1.csv`). This process forms the foundational data layer for Project IRIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06a8a79-a4a3-44a4-badb-2778647c9100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Successfully loaded raw data from '../data/DataSet-Obras-Publicas-23-07-2025.xlsx'.\n",
      "Initial shape: 180941 rows, 113 columns.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Environment Setup & Data Ingestion ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# Define the path to the raw Excel file\n",
    "excel_path = '../data/DataSet-Obras-Publicas-23-07-2025.xlsx'\n",
    "\n",
    "try:\n",
    "    # Load the Excel file using the openpyxl engine\n",
    "    df_raw = pd.read_excel(excel_path, engine='openpyxl')\n",
    "    print(f\"Successfully loaded raw data from '{excel_path}'.\")\n",
    "    print(f\"Initial shape: {df_raw.shape[0]} rows, {df_raw.shape[1]} columns.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"CRITICAL ERROR: The file was not found at the specified path: {excel_path}\")\n",
    "    print(\"Please ensure the raw data file exists before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468d11d9-2cad-4da2-833a-78fddfec26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing function `clean_infobras_data` defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: The Data Processing Pipeline Function ---\n",
    "\n",
    "def clean_infobras_data(raw_df):\n",
    "    \"\"\"\n",
    "    This function takes the raw INFOBRAS DataFrame and executes a complete\n",
    "    cleaning, auditing, and curation pipeline. (Version 3.0)\n",
    "    \n",
    "    Args:\n",
    "        raw_df (pd.DataFrame): The raw DataFrame loaded from the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A certified, analysis-ready DataFrame.\n",
    "    \"\"\"\n",
    "    df = raw_df.copy()\n",
    "    print(\"Cleaning pipeline initiated...\")\n",
    "\n",
    "    # --- Step 2.1: Shielding Text Data ---\n",
    "    print(\"  -> Step 2.1: Shielding text columns (stripping whitespace)...\")\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].str.strip()\n",
    "    \n",
    "    # --- Step 2.2: Standardizing Column Names ---\n",
    "    print(\"  -> Step 2.2: Standardizing column names to snake_case...\")\n",
    "    df.columns = (df.columns.str.lower()\n",
    "                  .str.replace(' ', '_', regex=False).str.replace('¿', '', regex=False)\n",
    "                  .str.replace('?', '', regex=False).str.replace('(', '', regex=False)\n",
    "                  .str.replace(')', '', regex=False).str.replace(':', '', regex=False)\n",
    "                  .str.replace('.', '', regex=False).str.normalize('NFKD')\n",
    "                  .str.encode('ascii', errors='ignore').str.decode('utf-8'))\n",
    "\n",
    "    # --- Step 2.3: Repairing Structural Defects ---\n",
    "    print(\"  -> Step 2.3: Repairing structure (renaming duplicated columns)...\")\n",
    "    rename_dict = {\n",
    "        'ruc1': 'supervisor_ruc', 'nombre_o_razon_social_de_la_empresa_o_consorcio1': 'supervisor_name',\n",
    "        'monto_del_contrato__en_soles1': 'supervisor_contract_amount_soles', 'tipo_de_documento_de_identidad1': 'resident_id_type',\n",
    "        'numero_de_documento1': 'resident_id_number', 'nombres_apellidos1': 'resident_full_name',\n",
    "        'colegiatura1': 'resident_professional_org', 'numero_de_colegiatura1': 'resident_professional_id',\n",
    "        'fecha_inicio_de_labores': 'resident_start_date', 'fecha_fin__de_labores': 'resident_end_date'\n",
    "    }\n",
    "    df = df.rename(columns=rename_dict)\n",
    "\n",
    "    # --- Step 2.4: Pruning Uninformative Columns ---\n",
    "    print(\"  -> Step 2.4: Pruning uninformative (mostly empty) columns...\")\n",
    "    cols_to_prune = ['fecha_de_aprobacion', 'otra_marca', 'tipo_de_certificado_de_inversion_publica', \n",
    "                     'numero_del_cipril_/cipgn', 'fecha_del_cipril_/_cipgn', 'monto_cipril_/cipgn']\n",
    "    df = df.drop(columns=cols_to_prune, errors='ignore')\n",
    "    \n",
    "    # --- Step 2.5: Transforming Data Types (Dates) ---\n",
    "    print(\"  -> Step 2.5: Transforming date columns...\")\n",
    "    date_cols = ['fecha_de_actualizacion', 'fecha_de_aprobacion_del_expediente', 'fecha_inicio_supervision', 'fecha_fin_supervision', \n",
    "                 'resident_start_date', 'resident_end_date', 'fecha_de_inicio_de_obra', 'fecha_finalizacion_programada_de_obra', \n",
    "                 'fecha_de_entrega_del_terreno', 'fecha_de_registro_de_avance', 'fecha_de_paralizacion', \n",
    "                 'fecha_finalizacion_reprogramada_de_obra', 'fecha_de_finalizacion_real', 'fecha_de_recepcion', \n",
    "                 'fecha_de_aprobacion_de_liquidacion_de_obra', 'fecha_de_transferencia']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], dayfirst=True, errors='coerce')\n",
    "            \n",
    "    # --- Step 2.6: Integrated Auditing & Correction ---\n",
    "    print(\"  -> Step 2.6: Auditing and correcting for temporal coherence...\")\n",
    "    inconsistent_mask = (df['fecha_de_finalizacion_real'].notna()) & (df['fecha_de_inicio_de_obra'].notna()) & (df['fecha_de_finalizacion_real'] < df['fecha_de_inicio_de_obra'])\n",
    "    num_inconsistent = inconsistent_mask.sum()\n",
    "    if num_inconsistent > 0:\n",
    "        df = df[~inconsistent_mask].copy()\n",
    "        print(f\"     - Removed {num_inconsistent} rows with incoherent dates.\")\n",
    "    \n",
    "    # --- Step 2.7: Transforming Data Types (Numerics) ---\n",
    "    print(\"  -> Step 2.7: Decontaminating and transforming numeric columns...\")\n",
    "    contaminated_numeric_cols = ['monto_viable/aprobado', 'monto_de_aprobacion_de_expediente_tecnico', 'tasa_de_cambio', 'monto_aprobado_en_soles', 'monto_del_contrato__en_soles', 'supervisor_contract_amount_soles', 'porcentaje_de_terreno_entregado', 'avance_fisico_programado_acumulado_%', 'avance_fisico_real_acumulado_%', 'monto_de_valorizacion_programado_acumulado', 'monto_de_valorizacion_ejecutado_acumulado', 'porcentaje_de_ejecucion_financiera', 'monto_de_ejecucion_financiera_de_la_obra', 'monto_de_adicionales_de_obra_en_soles', 'monto_de_adicionales_de_supervision_en_soles', 'monto_de_deductivos_de_obra_en_soles', 'costo_de_la_obra_en_soles', 'monto_total_devengado_del_proyecto']\n",
    "    for col in contaminated_numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace('S/.', '', regex=False).str.replace(',', '', regex=False).str.replace('%', '', regex=False).str.strip()\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # --- Step 2.8: Strategic Null Value Imputation ---\n",
    "    print(\"  -> Step 2.8: Handling null values strategically...\")\n",
    "    df['causal_de_paralizacion'] = df['causal_de_paralizacion'].fillna('Not Paralyzed')\n",
    "    df['corresponde_a_un_saldo_de_obra'] = df['corresponde_a_un_saldo_de_obra'].fillna('No')\n",
    "    key_categorical_cols = ['departamento', 'provincia', 'distrito', 'modalidad_de_ejecucion_de_la_obra', 'estado_de_ejecucion']\n",
    "    for col in key_categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in object_cols:\n",
    "        df[col] = df[col].fillna('Not Applicable')\n",
    "\n",
    "    # --- Step 2.9: Data Curation (Plausibility Filters) ---\n",
    "    print(\"  -> Step 2.9: Applying business plausibility filters (curation)...\")\n",
    "    initial_rows = len(df)\n",
    "    df = df[df['plazo_de_ejecucion_en_dias'] >= 1]\n",
    "    print(f\"     - Removed {initial_rows - len(df)} rows with non-positive timelines.\")\n",
    "    initial_rows = len(df)\n",
    "    df = df[df['plazo_de_ejecucion_en_dias'] <= 3650] # Capping at 10 years\n",
    "    print(f\"     - Removed {initial_rows - len(df)} rows with extreme timelines (> 10 years).\")\n",
    "\n",
    "    print(\"\\nPIPELINE COMPLETED: Cleaning, auditing, and curation finished!\")\n",
    "    return df\n",
    "\n",
    "print(\"Data processing function `clean_infobras_data` defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1cc4df-8acf-4e10-8e8e-daeea4577811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning pipeline initiated...\n",
      "  -> Step 2.1: Shielding text columns (stripping whitespace)...\n",
      "  -> Step 2.2: Standardizing column names to snake_case...\n",
      "  -> Step 2.3: Repairing structure (renaming duplicated columns)...\n",
      "  -> Step 2.4: Pruning uninformative (mostly empty) columns...\n",
      "  -> Step 2.5: Transforming date columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GAMERS\\AppData\\Local\\Temp\\ipykernel_13324\\4046884841.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], dayfirst=True, errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step 2.6: Auditing and correcting for temporal coherence...\n",
      "     - Removed 898 rows with incoherent dates.\n",
      "  -> Step 2.7: Decontaminating and transforming numeric columns...\n",
      "  -> Step 2.8: Handling null values strategically...\n",
      "  -> Step 2.9: Applying business plausibility filters (curation)...\n",
      "     - Removed 47887 rows with non-positive timelines.\n",
      "     - Removed 19 rows with extreme timelines (> 10 years).\n",
      "\n",
      "PIPELINE COMPLETED: Cleaning, auditing, and curation finished!\n",
      "\n",
      "----------------------------------------------------\n",
      "✅  PROCESS FINISHED SUCCESSFULLY\n",
      "File '../data/infobras_certified_v1.csv' has been exported.\n",
      "This file is clean, audited, curated, and ready for analysis.\n",
      "Final shape: 132137 rows, 108 columns.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Execute Pipeline and Export Certified Asset ---\n",
    "\n",
    "# Execute the complete cleaning, auditing, and curation pipeline\n",
    "df_certified = clean_infobras_data(df_raw)\n",
    "\n",
    "# Save the final, certified result to a new CSV file\n",
    "certified_path = '../data/infobras_certified_v1.csv'\n",
    "df_certified.to_csv(certified_path, index=False)\n",
    "\n",
    "print(\"\\n----------------------------------------------------\")\n",
    "print(\"✅  PROCESS FINISHED SUCCESSFULLY\")\n",
    "print(f\"File '{certified_path}' has been exported.\")\n",
    "print(\"This file is clean, audited, curated, and ready for analysis.\")\n",
    "print(f\"Final shape: {df_certified.shape[0]} rows, {df_certified.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df64d7-f7b5-417b-9ada-ad3070026f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
